{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Advanced Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Goals</b>\n",
    "\n",
    "- Follow up our lesson on ensemble methods with boosting, what it is and how it works.\n",
    "- Use the super Gradient Boosting Classifier/Regressor.\n",
    "- How to use high-powered tools in sklearn to optimize your models and minimize your work load and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boosting is an ensemble method where a model is comprised of a sequence of models, as opposed to a set of parallel models as with Random Forest.\n",
    "- Unlike bagging, boosting uses random subsets of training data <b>WITHOUT</b> replacement.\n",
    "- It is an iterative process. Begins by training simple model on the whole data, pinpoints the inaccuracies, and trains a new model to target those inaccuracies (misclassification rate, residuals.) The new models try to predict what the previous ones were unable to correctly predict. Repeat until reaching a stopping point parameter. The whole set of models is what's used to make predictions.\n",
    "- Boosting process:\n",
    "    - Randomly select a batch of data from training dataset without replacement to train \"weak learner.\"\n",
    "    - Randomly select a second batch of data from training dataset without replacement AND add around half of the samples that were misclassified from the previous model.\n",
    "    - Go back to the original training dataset and retrieve the data points in which the two models had differing classifications.\n",
    "    - Make predictions by combining the system of weak learners and takin the vote (classification) or avearge (regression.)\n",
    "    \n",
    "- Can be used both for regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The AdaBoost (Adaptive Boosting) algorithm fits sequential weak classifiers, which are classifiers that are slightly better than random chance. These classifiers are usually tree-based models with a lower depth level. Adaboost actually uses the whole training dataset instead of sample. The data is weighted in each iteration of modeling to help it learn from the mistakes of the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The weak learners in the AdaBoost algorithm are Decision Trees with one depth-level aka \"Decision Stumps.\" They literally only use one decision.\n",
    "\n",
    "- Each data point in the training data is assigned a weight. In the first model, every point has the same weight value which equal 1/number of values. \n",
    "\n",
    "- The first Decision Stump is fit on the whole data using weighted samples. Only works with binary clasification problems. The model outputs either a 1 or - 1, irregardless of the class labels in the target variable. \n",
    "\n",
    "- Error determined by the misclassification rate, which is 1 - accuracy score. Accuracy score of 0.71 means error rate of 0.29.\n",
    "\n",
    "- However error significantly changes when differents are introduced. \n",
    "\n",
    "- With weights, error = sum(w(i) * terror(i)) / sum(w). If terror is 1, then equals wrong prediction, 0 if correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost visually explained:\n",
    "\n",
    "![a](https://www.analyticsvidhya.com/wp-content/uploads/2015/11/bigd.png)\n",
    "\n",
    "Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box 1: Each data point has equal weighting is fit on a decision stump which is a vertical line.\n",
    "\n",
    "<br>\n",
    "\n",
    "Box 2: The three plus signs that were incorrectly classified in Box 1 have been enlarged (weighted) and the model has been retrained.\n",
    "\n",
    "<br>\n",
    "\n",
    "Box 3: Three minus signs have been given bigger weight values and the new model (horizontal line) has been fit to account for that.\n",
    "\n",
    "<br>\n",
    "\n",
    "Box 4: Combines the three Decision stump models, which vastly outperforms any of the three stumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of weights\n",
    "w = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "#List of actual values\n",
    "y = [1,  1, -1, 1, -1]\n",
    "#List of predictions\n",
    "p = [-1, 1, 1, 1, -1]\n",
    "#List or terrors\n",
    "t = [1, 0, 1, 0 , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular error rate calculation\n",
    "(1 + 0 + 1 + 0 + 0)/(1 + 1 + 1 + 1 + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error calculation with weights (same product as above)\n",
    "e = (0.2 * 1 + 0.2 * 0 + 0.2 * 1 + 0.2 * 0 + 0.2 * 0)/ (0.2 + 0.2 + 0.2 + 0.2 + 0.2)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part we pass in the error rate through this function: 0.5 * log((1-e)/e)\n",
    "\n",
    "This gives a coefficient: a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import numpy\n",
    "import numpy as np\n",
    "\n",
    "a = 0.5 * np.log((1 - e)/ e)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this value to update our new weights.\n",
    "\n",
    "Formula is old weight value times the exponent of the negative value of a times prediction times actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First value\n",
    "\n",
    "w1 = 0.2 * np.exp(-a * 1 * -1)\n",
    "w1\n",
    "\n",
    "w2 = 0.2 * np.exp(-a * 1 * 1)\n",
    "w2\n",
    "\n",
    "w3 = 0.2 * np.exp(-a * 1 * -1)\n",
    "w3\n",
    "\n",
    "w4 = 0.2 * np.exp(-a * 1 * 1)\n",
    "w4\n",
    "\n",
    "w5 = 0.2 * np.exp(-a * 1 * 1)\n",
    "w5\n",
    "\n",
    "print (w1, w2, w3, w4, w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights go up for wrong predictions and go down for correct ones.\n",
    "\n",
    "We're not finished yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we normalize the weight by diving each weight by the sum of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sum = w1 + w2 + w3 + w4 + w5\n",
    "\n",
    "#New weights\n",
    "w1 = w1/(weight_sum)\n",
    "w2 = w2/(weight_sum)\n",
    "w3 = w3/(weight_sum)\n",
    "w4 = w4/(weight_sum)\n",
    "w5 = w5/(weight_sum)\n",
    "\n",
    "print (w1, w2, w3, w4, w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our new weights which we'll use in the next round of modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the follow - up model, a second Decision Stump model is trained using our new weights. The weights are used to determine the split in the decision tree. This process continues until we reach the n_estimators parameter we set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the weights for the mis-classified data points forces the models to train more heavily on the data it incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost makes predictions by calculating the weighted average of the sequence of Decision Stumps. \n",
    "\n",
    "- When you pass in a new data point, the model predicts 1 or -1.\n",
    "\n",
    "- The weights of each model by each one's stage value. The prediction is derived from the sum of the of the weighted predictions. If sum > 0 then return the first class else return second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Five model predictions\n",
    "preds = np.array([-1, -1, 1, -1, 1])\n",
    "preds.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without weighting the prediction would be -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([.2, .4, .8, .3, .9])\n",
    "\n",
    "sum(weights * preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with weighted models equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warnings</b>\n",
    "\n",
    "- Requires rich data noisy data by design can negatively influence model.\n",
    "- Same goes with outliers, the model will chase outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding AdaBoost\n",
    "\n",
    "1. Visualize the decision boundaries of AdaBoost\n",
    "\n",
    "2. Use AdaBoost on the spotify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and visualize fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate fake data that is 400 x 2.\n",
    "data = make_classification(n_samples=400, n_features=2, n_informative=2, n_redundant=0, \n",
    "                    class_sep=.54, random_state = 8)\n",
    "\n",
    "df = pd.DataFrame(data[0], columns=[\"feature1\", \"feature2\"])\n",
    "#Add target variable to df \n",
    "df[\"target\"] = data[1]\n",
    "\n",
    "#Call scatter plot of feature1 vs feature2 with color-encoded target variable\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(11, 8))\n",
    "#Color encode target variable\n",
    "colors = df.target.map({0:\"b\", 1:\"r\"})\n",
    "plt.scatter(df.feature1, df.feature2, c = colors, s = 100, alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "\n",
    "#Fit a Decision Tree model with max_depth = 1 on the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision boundary function\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, alpha=0.4, s = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed dt model, features and colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train AdaBoost model on the same data and visualize it\n",
    "\n",
    "#Intialize AdaBoost model with 20 estimators\n",
    "\n",
    "\n",
    "#Fit model\n",
    "\n",
    "\n",
    "#Visualize model boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use AdaBoost on Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import spotify data\n",
    "\n",
    "spotify = pd.read_csv(\"../data/spotify_data.csv\", index_col=[0])\n",
    "spotify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast Decision Trees and AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Decision Stump\n",
    "\n",
    "\n",
    "#Intialize AdaBoost with 300 estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = \n",
    "\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split with test size = 0.33 and random_state = 12\n",
    "\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Decision Stump on training data and score it on training and testing sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Ada boost model on training data and score it on testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validate both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validate DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validate ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation shows AdaBoost doesn't really improve our model.\n",
    "\n",
    "Perhaps we chose the wrong estimator value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a validation curve to determine the best value for the estimator.\n",
    "\n",
    "<br>\n",
    "\n",
    "This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to time our code\n",
    "\n",
    "#Import time tool\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize time variable\n",
    "t = time()\n",
    "\n",
    "#Create list of estimator values\n",
    "estimators = \n",
    "\n",
    "#Intialize cross validation scores list\n",
    "cv_scores = []\n",
    "\n",
    "#Iterate over estimators values, fit models, and then append scores to cv_scores\n",
    "for est in estimators:\n",
    "    \n",
    "\n",
    "    \n",
    "#Print difference in time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Plot estimators versus scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive best estimator value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I told you there was a more efficient way to find the best parameter for a given model? The best depth or K-neighbors, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Sklearn tools\n",
    "\n",
    "Overview:\n",
    "\n",
    "- Grid search\n",
    "- Pipelinings\n",
    "- Imputation\n",
    "- Feature unions\n",
    "- Feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch\n",
    "\n",
    "Algorithm that tests every combination of model parameters to find the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use GridSearch to find the best K value for a KNN model and Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize parameter grid\n",
    "\n",
    "#Range of neighbors to test\n",
    "\n",
    "\n",
    "#Dictionary of parameter values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats the best cross validated accuracy score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple technique gives us the best K value.\n",
    "\n",
    "We can use the best model from grid_knn to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input \n",
    "x = [[0.2, .15, 0.68, 0.05, 0.328]]\n",
    "\n",
    "#Make prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick exercise:\n",
    "<br>\n",
    "Use grid search to determine the best depth value in a decision tree. Use depths from 2 - 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Range of neighbors to test\n",
    "\n",
    "\n",
    "#Dictionary of parameter values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV in GridSearchCV stands for cross validation which means we have to set a cv and scoring value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score for DT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter for DT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our grids have been one-dimensional, now let's try using multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param grid with test different split criteria as well.\n",
    "param_grid_dt = \n",
    "\n",
    "param_grid_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's going to cross-validate every combination between the criterion parameters and depth parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = \n",
    "#Fit grid on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many models did this grid search function conduct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid_dt[\"min_samples_split\"] = \n",
    "param_grid_dt[\"max_features\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = \n",
    "\n",
    "#Time the code \n",
    "\n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "\n",
    "\n",
    "#Print time difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously grid search takes a long time and in some case can cause memory errors. This is where RandomizedSearchCV comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions just like GridSearchCV, except we have to choose a value n_iter which is the random number of combinations we testing and set param_distributions instead of param_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize RandomizedSearchCV grid with n_iter = 20\n",
    "grid_dt = \n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced run time by 96%!\n",
    "\n",
    "But now let's see if we sacrificed performance. Remember our previous best score was 0.667."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check best score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By going from about 24 seconds to 0.87 we sacrificed 0.009 percentage points in accuracy.\n",
    "\n",
    "Good trade-off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Let's go back to using the KNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we need to scale our data for the KNN algorithm right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data and fit it a Grid search function it.\n",
    "\n",
    "#Intialize scalar\n",
    "\n",
    "\n",
    "#Fit and transform scaler on the data\n",
    "Xs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_knn_s = \n",
    "\n",
    "#Fit grid on scaled data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best K value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n",
    "#First transform predict using scaler\n",
    "\n",
    "\n",
    "\n",
    "#Pass in xs to grid model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass scaler and knn classifier objects into make_pipeline function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new param_grid\n",
    "neighbors_range = range(2, 21)\n",
    "param_grid_knn = {}\n",
    "param_grid_knn[\"kneighborsclassifier__n_neighbors\"] = neighbors_range\n",
    "param_grid_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pass in pipe into GridSearchCV function, \n",
    "grid_knn_pipe = \n",
    "\n",
    "#Fit on original versions of data\n",
    "\n",
    "\n",
    "#Best scores and params\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in the pipe object into a cross_val_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cross-validation process using Pipeline\n",
    "pipe = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class exercise: Use grid search to model the 2016 Democratic primary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data files\n",
    "primary = pd.read_csv(\"../data/primary_data//primary_results.csv\")\n",
    "county = pd.read_csv(\"../data/primary_data/county_facts.csv\")\n",
    "county_dict = pd.read_csv(\"../data/primary_data/county_facts_dictionary.csv\")\n",
    "\n",
    "subset_col_index = [0,3,5,9,10,12,18,20,23,25,33,34,53]\n",
    "\n",
    "county = county.iloc[:,subset_col_index].copy()\n",
    "\n",
    "subset_cols = [\"fips\",\"population\", \"pop_change\", \"senior_pop_per\", \"female_pop_per\", \"black_pop_per\",\n",
    "               \"white_pop_per\", \"foreign_pop_per\", \"college_degree_pop_\", \"commute_time\", \"median_income\",\n",
    "               \"poverty_rate\", \"pop_density\"]\n",
    "\n",
    "col_dict = dict(zip(county.columns, subset_cols))\n",
    "#Use dictionary to rename the columns\n",
    "county.rename(columns=col_dict, inplace=True)\n",
    "primary.dropna(inplace=True)\n",
    "bern = primary[primary.candidate== \"Bernie Sanders\"]\n",
    "hill = primary[primary.candidate== \"Hillary Clinton\"]\n",
    "bern = bern[[\"fips\", \"candidate\", \"votes\"]]\n",
    "dem = pd.merge(hill, bern, on=\"fips\")\n",
    "dem.rename(columns={\"votes_x\":\"clinton_votes\", \"votes_y\":\"sanders_votes\"}, inplace=True)\n",
    "dem[\"winner\"] = dem.clinton_votes - dem.sanders_votes\n",
    "def vote_winner(x):\n",
    "    if x >0:\n",
    "        return \"H\"\n",
    "    elif x == 0:\n",
    "        return \"TIE\"\n",
    "    else:\n",
    "        return \"B\"\n",
    "    \n",
    "dem[\"winner\"] = dem.winner.apply(vote_winner)\n",
    "\n",
    "dem = dem[dem.winner!= \"TIE\"]\n",
    "dem = dem[[\"fips\", \"winner\"]]\n",
    "df = pd.merge(county, dem, on=\"fips\")\n",
    "df.set_index(\"fips\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answers go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a pipeline using regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston[\"data\"])\n",
    "df.columns = boston[\"feature_names\"]\n",
    "df[\"MEDV\"] = boston[\"target\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = df.drop(\"MEDV\", axis =1)\n",
    "y = df.MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a Pipeline Class instead of function to establish pipeline\n",
    "pipe_poly = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a few features from X\n",
    "XX = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize range values for poly\n",
    "poly_range = \n",
    "\n",
    "#Intialize grid dictionary\n",
    "param_grid_poly = {}\n",
    "\n",
    "#Input grid values\n",
    "param_grid_poly[\"polynomialfeatures__degree\"] = \n",
    "\n",
    "#Establish the grid\n",
    "poly_grid ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Grid Search with Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_poly = Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
    "                            ('ridgeregression', Ridge())]) \n",
    "\n",
    "param_grid_ridge = {'polynomialfeatures__degree': [1, 2, 3, 4, 5],\n",
    "              'ridgeregression__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_ridge = RandomizedSearchCV(pipe_poly, param_distributions=param_grid_ridge, \n",
    "                                n_iter = 5 , cv = 5, scoring='neg_mean_squared_error')\n",
    "grid_ridge.fit(XX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (grid_ridge.best_params_, grid_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "<b> Boosting: </b>\n",
    "\n",
    "- https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n",
    "\n",
    "- http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "- https://www.youtube.com/watch?v=Rm6s6gmLTdg&list=PLaslQpv_LmSKxSCBPdKWEI7lLHrTCeewl\n",
    "\n",
    "- https://www.coursera.org/learn/practical-machine-learning/lecture/9mGzA/boosting\n",
    "\n",
    "<b> Grid Search and Pipelines </b>\n",
    "\n",
    "- https://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html\n",
    "- https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "- https://chrisalbon.com/machine-learning/pipelines_with_parameter_optimization.html\n",
    "- https://chrisalbon.com/machine-learning/hyperparameter_tuning_using_random_search.html\n",
    "- https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "- https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class work.\n",
    "\n",
    "Choose one of the following dataset to work on modeling for the rest of class using the new models and tools we've learned in the past couple weeks.\n",
    "\n",
    "<br>\n",
    "Spotify, Dem Primary, KC Housing, Movie Metadata, HR Employee, Breast Cancer, Default, Mushrooms, Red win quality, Zillow starter, or Pokemon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
