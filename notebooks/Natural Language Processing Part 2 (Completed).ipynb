{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to pick up where we left off\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Finish text classification lesson by using stemming and lemmatization in our vectorizers\n",
    "- Build a simple text summarizer\n",
    "- How to find similar documents with cosine similarity and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from time import time\n",
    "import pandas as pd\n",
    "pd.set_option(\"max.colwidth\", 500)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "\n",
    "path = \"../data/NLP_data/yelp.csv\"\n",
    "\n",
    "yelp = pd.read_csv(path, encoding='unicode-escape')\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.816691\n",
      "1    0.183309\n",
      "Name: stars, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print y.value_counts(normalize=True)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the analyzer section of the CountVectorizer doc strings\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer argument allows us to upload our function to transform/tokenize the words in our corpura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of stems\n",
    "def word_tokenize_stem(text):\n",
    "    #Transform and tokenize words using TextBlob\n",
    "    words = TextBlob(text).words\n",
    "    #Intialize stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    #Return a list of the stems\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "\n",
    "# define a function that accepts text and returns a list of lemons (noun version)\n",
    "def word_tokenize_lemma(text):\n",
    "    #Transform and tokenize words using TextBlob\n",
    "    words = TextBlob(text).words\n",
    "    #Return a list of lemons\n",
    "    return [word.lemmatize() for word in words]\n",
    "\n",
    "# define a function that accepts text and returns a list of lemons (verb version)\n",
    "def word_tokenize_lemma_verb(text):\n",
    "    words = TextBlob(text).words\n",
    "    #Return a list of lemons    \n",
    "    return [word.lemmatize(pos=\"v\") for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our three new functions with both count and tfidf vectorizers. \n",
    "<br>\n",
    "- First let's create a function that takes in an initialized but unfit vectorizer as an argument.\n",
    "- Fit and transforms training data using the vectorizer\n",
    "- Transforms the testing data\n",
    "- Fits naive bayes model on training data.\n",
    "- Evaluate it on the training and testing data.\n",
    "- Prints the number of features and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_model_evaluator(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    print \"Features: \", X_train_dtm.shape[1]\n",
    "    print \"Training Score: \", nb.score(X_train_dtm, y_train)\n",
    "    print \"Testing Score: \", nb.score(X_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.970626631854\n",
      "Testing Score:  0.924657534247\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.904109589041\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  19431\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.906066536204\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma_verb\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma_verb)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these results? Let's try it again with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.816906005222\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the tfidf vectorizers compare to counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search time. Let's grid search objects that incorporate all of the analyzer functions for count and tfidf vectorizers. In addition we'll do the same for randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv[\"countvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_cv[\"countvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_cv[\"countvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__binary\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search object\n",
    "\n",
    "grid_cv = GridSearchCV(pipe_cv, param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_cv.fit(X, y)\n",
    "#Print time elapsed\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "print grid_cv.best_params_\n",
    "#Best score\n",
    "print grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidfvectorizer gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf[\"tfidfvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_tf[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_tf[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search object\n",
    "\n",
    "grid_tf = GridSearchCV(pipe_tf, param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_tf.fit(X, y)\n",
    "#Print time elapsed\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.130045176\n"
     ]
    }
   ],
   "source": [
    "#Randomized grid search with n_iter = 5\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n",
    "                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'countvectorizer__lowercase': True, 'countvectorizer__analyzer': 'word', 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__binary': False, 'countvectorizer__max_features': 7500}\n",
      "0.931962799804\n"
     ]
    }
   ],
   "source": [
    "#Best params\n",
    "print randsearch_cv.best_params_\n",
    "#Best score\n",
    "print randsearch_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidfvectorizer randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized grid search with n_iter = 10\n",
    "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n",
    "                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_tf.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best params\n",
    "print randsearch_tf.best_params_\n",
    "#Best score\n",
    "print randsearch_tf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wraps up text classification. Now onto the rest of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing text\n",
    "\n",
    "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the greatest difficulties that companies wishing to become more analytical have encountered over the last several years is finding good analysts and data scientists. A considerable amount of printer’s ink has been spilled into articles over this issue. Many of them mention consultants’ or analyst firms’ projections about how many quantitative analysts or data scientists will be needed in our society and conclude that it will be incredibly difficult to find them.\\n\\nI always thought th...</td>\n",
       "      <td>What Data Scientist Shortage? Get Serious and Get Talent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Within soccer’s nascent analytics movement, one metric dominates most discussions. It’s called Expected Goals or xG. Models for calculating xG differ, but the underlying concept is the same. In a nutshell, xG takes a shot’s characteristics – distance from goal, angle from goal, root cause, etc. – and assigns a probability that said shot will result in a goal. Accounting for these probabilities reveals which team creates better scoring opportunities. Given a season of data, xG analysis is a p...</td>\n",
       "      <td>xG, Soccer Analytics of Bundesliga in R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The company’s adjacent market opportunities are growing at a CAGR of 18% for the next five years.\\n\\nQualcomm (NASDAQ: QCOM) announced a few days ago that its subsidiary Qualcomm Technologies will offer OEMs its first machine learning SDK for running their own neural network models on devices powered by Snapdragon 820 SoCs. The devices include smartphones, cars and drones among many others. Gary Brotman, director of product management, Qualcomm Technologies, said:\\n\\nWith the introduction of...</td>\n",
       "      <td>Qualcomm: Taking Artificial Intelligence To A New Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Web, Tech Companies Use GPUs to Put Deep Learning at Your Fingertips\\n\\nGPUs have helped researchers spark a deep-learning revolution that’s given computers super-human capabilities.\\n\\nThey’ve already enabled breakthrough results on the industry-standard ImageNet benchmark. They’re powering Facebook’s “Big Sur” deep learning computing platform. They’re also accelerating major advances in deep learning across a broad range of fields.\\n\\nGPUs have become the go-to technology for training ...</td>\n",
       "      <td>How Companies Use GPUs to Put Deep Learning at Your Fingertips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House technology policy adviser Kristen Honey urged government and industry IT leaders to support the open data movement and showcase their work at two upcoming data innovation events.\\n\\nSpeaking Wednesday to a standing-room-only audience at the annual Data Innovation Summit in Washington, Honey highlighted a number of the administration’s open data initiatives, dating back to 2009, that are leading to innovative advances in medicine, agriculture, energy, transportation and education....</td>\n",
       "      <td>White House official urges IT leaders to join open data efforts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  One of the greatest difficulties that companies wishing to become more analytical have encountered over the last several years is finding good analysts and data scientists. A considerable amount of printer’s ink has been spilled into articles over this issue. Many of them mention consultants’ or analyst firms’ projections about how many quantitative analysts or data scientists will be needed in our society and conclude that it will be incredibly difficult to find them.\\n\\nI always thought th...   \n",
       "1  Within soccer’s nascent analytics movement, one metric dominates most discussions. It’s called Expected Goals or xG. Models for calculating xG differ, but the underlying concept is the same. In a nutshell, xG takes a shot’s characteristics – distance from goal, angle from goal, root cause, etc. – and assigns a probability that said shot will result in a goal. Accounting for these probabilities reveals which team creates better scoring opportunities. Given a season of data, xG analysis is a p...   \n",
       "2  The company’s adjacent market opportunities are growing at a CAGR of 18% for the next five years.\\n\\nQualcomm (NASDAQ: QCOM) announced a few days ago that its subsidiary Qualcomm Technologies will offer OEMs its first machine learning SDK for running their own neural network models on devices powered by Snapdragon 820 SoCs. The devices include smartphones, cars and drones among many others. Gary Brotman, director of product management, Qualcomm Technologies, said:\\n\\nWith the introduction of...   \n",
       "3  How Web, Tech Companies Use GPUs to Put Deep Learning at Your Fingertips\\n\\nGPUs have helped researchers spark a deep-learning revolution that’s given computers super-human capabilities.\\n\\nThey’ve already enabled breakthrough results on the industry-standard ImageNet benchmark. They’re powering Facebook’s “Big Sur” deep learning computing platform. They’re also accelerating major advances in deep learning across a broad range of fields.\\n\\nGPUs have become the go-to technology for training ...   \n",
       "4  White House technology policy adviser Kristen Honey urged government and industry IT leaders to support the open data movement and showcase their work at two upcoming data innovation events.\\n\\nSpeaking Wednesday to a standing-room-only audience at the annual Data Innovation Summit in Washington, Honey highlighted a number of the administration’s open data initiatives, dating back to 2009, that are leading to innovative advances in medicine, agriculture, energy, transportation and education....   \n",
       "\n",
       "                                                             title  \n",
       "0         What Data Scientist Shortage? Get Serious and Get Talent  \n",
       "1                          xG, Soccer Analytics of Bundesliga in R  \n",
       "2          Qualcomm: Taking Artificial Intelligence To A New Level  \n",
       "3   How Companies Use GPUs to Put Deep Learning at Your Fingertips  \n",
       "4  White House official urges IT leaders to join open data efforts  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in data\n",
    "\n",
    "path = \"../data/NLP_data/ds_articles.csv\"\n",
    "\n",
    "#We're only be using the text and title columns\n",
    "articles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\n",
    "\n",
    "#Drop nulls\n",
    "articles.dropna(inplace=True)\n",
    "\n",
    "#Reset index\n",
    "articles.reset_index(inplace=True, drop=True)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1418 entries, 0 to 1417\n",
      "Data columns (total 2 columns):\n",
      "text     1418 non-null object\n",
      "title    1418 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Info\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize tfidf with stop_words = english, max_features = 1000, and stem analzyer \n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=1000,\n",
    "                        analyzer=word_tokenize_stem)\n",
    "\n",
    "#Fit and transform the text using the tfidf vectorizer\n",
    "text = articles.text\n",
    "dtm = tfidf.fit_transform(text)\n",
    "\n",
    "#Assign tokens to features\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of features and their idf scores\n",
    "idfscores = pd.DataFrame()\n",
    "idfscores[\"tokens\"] = features\n",
    "idfscores[\"scores\"] = tfidf.idf_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>⭐️</td>\n",
       "      <td>7.564560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>var</td>\n",
       "      <td>6.311798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.772801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.961871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>blockchain</td>\n",
       "      <td>4.594146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>split</td>\n",
       "      <td>4.365887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>pdf</td>\n",
       "      <td>4.365887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>p</td>\n",
       "      <td>4.365887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>t</td>\n",
       "      <td>4.197265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>https</td>\n",
       "      <td>4.197265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens    scores\n",
       "999          ⭐️  7.564560\n",
       "936         var  6.311798\n",
       "2           0.0  5.772801\n",
       "4           1.0  4.961871\n",
       "133  blockchain  4.594146\n",
       "820       split  4.365887\n",
       "638         pdf  4.365887\n",
       "624           p  4.365887\n",
       "851           t  4.197265\n",
       "423       https  4.197265"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten most imporant words\n",
    "idfscores.sort_values(by=\"scores\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>to</td>\n",
       "      <td>1.012766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>the</td>\n",
       "      <td>1.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>of</td>\n",
       "      <td>1.020649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a</td>\n",
       "      <td>1.024975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>and</td>\n",
       "      <td>1.025697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>in</td>\n",
       "      <td>1.033683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>for</td>\n",
       "      <td>1.049848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>is</td>\n",
       "      <td>1.049848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>that</td>\n",
       "      <td>1.067786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>with</td>\n",
       "      <td>1.070051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tokens    scores\n",
       "895     to  1.012766\n",
       "873    the  1.015625\n",
       "598     of  1.020649\n",
       "20       a  1.024975\n",
       "72     and  1.025697\n",
       "440     in  1.033683\n",
       "356    for  1.049848\n",
       "471     is  1.049848\n",
       "872   that  1.067786\n",
       "978   with  1.070051"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten least imporant words\n",
    "idfscores.sort_values(by=\"scores\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's our summarizer function that will randomly select an article to summarize. By summarize, I mean show the top five words with the highest tfidf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    #Randomly choose index value\n",
    "    index = np.random.choice(articles.index, 1)[0]\n",
    "    article = text.iloc[index]\n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(article).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[index, features.index(word)]\n",
    "            \n",
    "   # print words with the top 5 TF-IDF scores\n",
    "    print 'TOP SCORING WORDS:'\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print word   \n",
    "        \n",
    "    #Print title of article\n",
    "    print \"\\n\", articles.title[index]\n",
    "    \n",
    "    #Print the text of article\n",
    "#     print article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "learn\n",
      "network\n",
      "neural\n",
      "to\n",
      "deep\n",
      "\n",
      "Over 150 of the Best Machine Learning, NLP, and Python Tutorials I’ve Found\n"
     ]
    }
   ],
   "source": [
    "#Give it a go\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity with Cosine Similarity and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "![ew](https://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png?w=697)\n",
    "<br><br>\n",
    "\" Cosine similarity metric finds the normalized dot product of the two attributes. By determining the cosine similarity, we would effectively try to find the cosine of the angle between the two objects. The cosine of 0° is 1, and it is less than 1 for any other angle.\n",
    "\n",
    "It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in (0,1). One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors.\"\n",
    "<br>\n",
    "Source: [Dataaspirant](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972\n"
     ]
    }
   ],
   "source": [
    "#Diy cosine similarity function\n",
    "\n",
    "def square_rooted(x):\n",
    "\n",
    "    return round(np.sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity_function(x,y):\n",
    "\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    " \n",
    "vec1 = [3, 45, 7, 2]\n",
    "vec2 = [2, 54, 13, 15]\n",
    "print cosine_similarity_function(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive matrix of similarities between all the data science articles documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cosine distance for each pair of documents\n",
    "dist = cosine_similarity(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1408</th>\n",
       "      <th>1409</th>\n",
       "      <th>1410</th>\n",
       "      <th>1411</th>\n",
       "      <th>1412</th>\n",
       "      <th>1413</th>\n",
       "      <th>1414</th>\n",
       "      <th>1415</th>\n",
       "      <th>1416</th>\n",
       "      <th>1417</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.497135</td>\n",
       "      <td>0.414230</td>\n",
       "      <td>0.371877</td>\n",
       "      <td>0.536993</td>\n",
       "      <td>0.580819</td>\n",
       "      <td>0.571139</td>\n",
       "      <td>0.371093</td>\n",
       "      <td>0.564958</td>\n",
       "      <td>0.714021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550215</td>\n",
       "      <td>0.497197</td>\n",
       "      <td>0.563890</td>\n",
       "      <td>0.609730</td>\n",
       "      <td>0.579023</td>\n",
       "      <td>0.478392</td>\n",
       "      <td>0.614185</td>\n",
       "      <td>0.427680</td>\n",
       "      <td>0.558389</td>\n",
       "      <td>0.479276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384127</td>\n",
       "      <td>0.307917</td>\n",
       "      <td>0.384483</td>\n",
       "      <td>0.458394</td>\n",
       "      <td>0.420705</td>\n",
       "      <td>0.268821</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.431185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469065</td>\n",
       "      <td>0.423733</td>\n",
       "      <td>0.436927</td>\n",
       "      <td>0.426546</td>\n",
       "      <td>0.405166</td>\n",
       "      <td>0.392849</td>\n",
       "      <td>0.462603</td>\n",
       "      <td>0.314531</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.319652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.414230</td>\n",
       "      <td>0.384127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.359263</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.475440</td>\n",
       "      <td>0.351316</td>\n",
       "      <td>0.503942</td>\n",
       "      <td>0.405210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484540</td>\n",
       "      <td>0.386657</td>\n",
       "      <td>0.372080</td>\n",
       "      <td>0.371335</td>\n",
       "      <td>0.377365</td>\n",
       "      <td>0.340714</td>\n",
       "      <td>0.434074</td>\n",
       "      <td>0.386217</td>\n",
       "      <td>0.407741</td>\n",
       "      <td>0.302086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.371877</td>\n",
       "      <td>0.307917</td>\n",
       "      <td>0.359263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.299243</td>\n",
       "      <td>0.523789</td>\n",
       "      <td>0.325652</td>\n",
       "      <td>0.289293</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>0.347880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361604</td>\n",
       "      <td>0.381003</td>\n",
       "      <td>0.459683</td>\n",
       "      <td>0.345898</td>\n",
       "      <td>0.325591</td>\n",
       "      <td>0.317953</td>\n",
       "      <td>0.450762</td>\n",
       "      <td>0.324969</td>\n",
       "      <td>0.334661</td>\n",
       "      <td>0.284952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.536993</td>\n",
       "      <td>0.384483</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.299243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460843</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.361754</td>\n",
       "      <td>0.465307</td>\n",
       "      <td>0.519213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421212</td>\n",
       "      <td>0.399286</td>\n",
       "      <td>0.436799</td>\n",
       "      <td>0.447065</td>\n",
       "      <td>0.426142</td>\n",
       "      <td>0.395756</td>\n",
       "      <td>0.453870</td>\n",
       "      <td>0.347057</td>\n",
       "      <td>0.426417</td>\n",
       "      <td>0.355321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1418 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.000000  0.497135  0.414230  0.371877  0.536993  0.580819  0.571139   \n",
       "1  0.497135  1.000000  0.384127  0.307917  0.384483  0.458394  0.420705   \n",
       "2  0.414230  0.384127  1.000000  0.359263  0.362200  0.451380  0.475440   \n",
       "3  0.371877  0.307917  0.359263  1.000000  0.299243  0.523789  0.325652   \n",
       "4  0.536993  0.384483  0.362200  0.299243  1.000000  0.460843  0.439024   \n",
       "\n",
       "       7         8         9       ...         1408      1409      1410  \\\n",
       "0  0.371093  0.564958  0.714021    ...     0.550215  0.497197  0.563890   \n",
       "1  0.268821  0.473777  0.431185    ...     0.469065  0.423733  0.436927   \n",
       "2  0.351316  0.503942  0.405210    ...     0.484540  0.386657  0.372080   \n",
       "3  0.289293  0.426559  0.347880    ...     0.361604  0.381003  0.459683   \n",
       "4  0.361754  0.465307  0.519213    ...     0.421212  0.399286  0.436799   \n",
       "\n",
       "       1411      1412      1413      1414      1415      1416      1417  \n",
       "0  0.609730  0.579023  0.478392  0.614185  0.427680  0.558389  0.479276  \n",
       "1  0.426546  0.405166  0.392849  0.462603  0.314531  0.365535  0.319652  \n",
       "2  0.371335  0.377365  0.340714  0.434074  0.386217  0.407741  0.302086  \n",
       "3  0.345898  0.325591  0.317953  0.450762  0.324969  0.334661  0.284952  \n",
       "4  0.447065  0.426142  0.395756  0.453870  0.347057  0.426417  0.355321  \n",
       "\n",
       "[5 rows x 1418 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it a dataframe\n",
    "dist_df = pd.DataFrame(dist)\n",
    "\n",
    "#Shape\n",
    "dist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index position of article\n",
    "index = 239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Popular TV Shows on Data Science and Artificial Intelligence\n",
      "\n",
      ", ************************************************ \n",
      "Introduction\n",
      "\n",
      "The development of full artificial intelligence could spell the end of human race. – Stephen Hawking\n",
      "\n",
      "The world is now rapidly moving towards achieving this finest technology breakthrough ever. It is expected that AI would enrich humans with more power and opportunities. Another group of people (including Stephen Hawking and Elon Musk) believe that this might lead to human destruction (if not handled carefully).\n",
      "\n",
      "I think, it’s too early for us to envisage such uncertain future. Good news is, companies like Google, Microsoft, Baidu have already started creating products based on AI. It won’t be long enough to experience the influence of AI in our daily lives.\n",
      "\n",
      "Accidentally, my exploration of AI started with movie ‘Her’. The influence was so powerful that I ended up creating an infographic on 10 Movies on Data Science and Machine Learning. May be a ~ 2 hours movie didn’t nourish my appetite well.\n",
      "\n",
      "Few days later, I came across a TV Series ‘Intelligence’. So far, this is one of the best show I’ve ever watched. Later, I found many other shows which beautifully describes a future world driven by data, technology, numbers and artificial intelligence. Here I’ve shared the best of them.\n",
      "\n",
      "Below are 10 Popular TV Shows on Data Science and Artificial Intelligence ( in no particular order). If you haven’t seen any of them, I’d suggest you to begin with ‘Intelligence’.\n",
      "\n",
      "If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.\n"
     ]
    }
   ],
   "source": [
    "#Assign titles column to titles variable\n",
    "\n",
    "titles = articles.title\n",
    "\n",
    "\n",
    "\n",
    "#Print title\n",
    "print titles[index]\n",
    "\n",
    "#print article\n",
    "\n",
    "print \"\\n, ************************************************ \\n\", text[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take the index value and use it grab the column of the scores between every article and the one at index 935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass\n",
    "dist_column = dist_df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the index values of the 5 \n",
    "\n",
    "closest_index = dist_column.nlargest(6).index[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Must Watch Movies on Data Science and Machine Learning\n",
      "10 Must Watch Movies on Data Science and Machine Learning\n",
      "Why We Need More Women Taking Part In The AI Revolution\n",
      "Why algorithms will be at the core of our AI-powered future, and why you should care\n",
      "The Non-Technical Guide to Machine Learning & Artificial Intelligence\n"
     ]
    }
   ],
   "source": [
    "#Pass index values into titles and print them\n",
    "\n",
    "for i in titles.iloc[closest_index].tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1116    Introduction\\n\\nSome members of our team (including me) live by just 2 passions in life – Data Science & Movies! For us, slicing and dicing movies over Monday morning coffee is part of warming up ritual.\\n\\nSo, we decided to do a poll among ourselves on the best movies related to data science and machine learning. We also thought that we would release the outcome of the results in form of an infographic.\\n\\nNeedless to say there were heated debates and a few disappointed faces in our office!...\n",
       "1157    Introduction\\n\\nSome members of our team (including me) live by just 2 passions in life – Data Science & Movies! For us, slicing and dicing movies over Monday morning coffee is part of warming up ritual.\\n\\nSo, we decided to do a poll among ourselves on the best movies related to data science and machine learning. We also thought that we would release the outcome of the results in form of an infographic.\\n\\nNeedless to say there were heated debates and a few disappointed faces in our office!...\n",
       "1000    Lolita Taub\\n\\nBy Samantha Walravens & Heather Cabot\\n\\nIn 2011, entrepreneur and investor Marc Andreessen wrote his famous ,\"Why Software Is Eating the World\" in the Wall Street Journal. Today, that story would more likely read, \"Why Artificial Intelligence Is Eating the World.\" The market for artificial intelligence (AI) technologies-- from voice and image recognition to chat bots to self-driving cars-- is hot. A Narrative Science survey found last year that 38% of enterprises are already ...\n",
       "1130    For the second year in a row, We Are Social had the privilege of presenting at Vivid Sydney this year. Already one of the world’s leading festivals, Vivid is a bit like SXSW: an amalgam of inspiring people, creative work, and fresh ideas across a variety of themes and topics.\\n\\nFor our 2017 keynote, I joined forces with We Are Social’s Sydney MD, Suzie Shaw, to explore the impact that algorithms and machine learning are having on every aspect of our lives, and what that means for marketing....\n",
       "1009    The Non-Technical Guide to Machine Learning & Artificial Intelligence\\n\\nI have a challenge for you.\\n\\nIn a few seconds, I want you to stop reading this article, and follow the instructions below.\\n\\nHere you go:\\n\\n2. Hit “crtl + f”\\n\\n3. Search “artificial intelligence” or “machine learning”\\n\\n4. If you don't find any matches, publicly shame me on Twitter.\\n\\nI hope my point is obvious.\\n\\nMachine learning and artificial intelligence (ML and AI) have seized Tech mindshare in a way few to...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass index values into titles and but don't print\n",
    "text.iloc[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "It is standard practice to cluster with tfidf data instead of the count vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize clustering algorithm with 4 clusters and fit it on dtm\n",
    "\n",
    "km4 = KMeans(n_clusters=4)\n",
    "#Fit algorithm\n",
    "km4.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020503208601170619"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dtm, km4.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign labels to articles dataframe \n",
    "\n",
    "articles[\"cluster\"] = km4.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 randomly selected headlines from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribe to read\n",
      "Building and Scaling the Internet of Things with MongoDB at Vivint\n",
      "Here’s a Sneak Peak Inside IBM’s Blockchain System\n",
      "Five New Machine Learning Tools To Make Your Software Intelligent\n",
      "7 Visualizations You Should Learn in R\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in articles[articles.cluster == 0].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPMorgan's massive guide to machine learning jobs in finance\n",
      "The Future Of Health Care Is In Data Analytics\n",
      "Data Science for Beginners: Fantastic Introductory Video Series from Microsoft\n",
      "How data and Artificial Intelligence are changing publishing\n",
      "Breaking Down The Gender Gap In Data Science\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1\n",
    "for i in articles[articles.cluster == 1].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting an eye over the 2017 fintech industry to prepare for change\n",
      "Building a smarter robot with deep learning and new algorithms\n",
      "Machine learning is leading the way to a smarter internet of things\n",
      "The 'Godfather of AI' on making machines clever and whether robots really will learn to kill us all?\n",
      "Monitoring Side-Channel Signals Could Detect Malicious Software on IoT Devices\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2\n",
    "for i in articles[articles.cluster == 2].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Introduction to Model-Based Machine Learning\n",
      "If you want to learn Data Science, take a few of these statistics classes\n",
      "Gaussian predictive process models in Stan\n",
      "Asynchronous methods for deep reinforcement learning\n",
      "Deep learning for neuroimaging: a validation study\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3\n",
    "for i in articles[articles.cluster == 3].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think the clusters are? Is it easy decipher? Ignore the silhouette score, does it pass the eye test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this exercise again but this time we'll cluster the cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize clustering algorithm with 4 clusters\n",
    "km4 = KMeans(n_clusters=4)\n",
    "\n",
    "#fit it on dist array\n",
    "\n",
    "km4.fit(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28531640871418196"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dist, km4.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 randomly selected headlines from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign new labels to data frame\n",
    "\n",
    "articles[\"cluster_dist\"] = km4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The many (emerging) faces of machine learning\n",
      "Humans need new skills for post-AI world, say MPs\n",
      "How AI Is Already Changing Business\n",
      "Research consortium building scaffolding for Precision Medicine Initiative’s All of Us\n",
      "False Positives Are a True Negative: Using Machine Learning to Improve Accuracy\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in articles[articles.cluster_dist == 0].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel's Python distribution turbocharges data science\n",
      "Splice Machine Announces Move to Open Source, Offers Early Access to Developers\n",
      "What Cost is each State Obsessed with\n",
      "The Strange Loop in Deep Learning – Intuition Machine – Medium\n",
      "The 6 top machine learning trends\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1\n",
    "for i in articles[articles.cluster_dist == 1].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Next Tsunami AI Blockchain IOT and Our Swarm Evolutionary Singula…\n",
      "Okhuge.com\n",
      "Computing's Machine Learning Tutorials gain support from top companies\n",
      "songrotek/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!\n",
      "Predicting Loan Credit Risk using Apache Spark Machine Learning Random Forests\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2\n",
    "for i in articles[articles.cluster_dist == 2].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Happens When a Robot Tries to Name Paint Colors?\n",
      "A guide to AI, machine learning and new workflow technologies at HIMSS17 Part 1: Machine learning and workflow\n",
      "Cognitive Analytics Answers the Question: What's Interesting in Your Data?\n",
      "5 Amazing Things Big Data Helps Us To Predict Now -- Plus What's On The Horizon\n",
      "Data Science: The One Skill that Ensures Future Success\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3\n",
    "for i in articles[articles.cluster_dist == 3].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "\n",
    "My fake news classifer article: https://opendatascience.com/blog/how-to-build-a-fake-news-classification-model/\n",
    "<br>\n",
    "My data science topic modeling article: https://opendatascience.com/blog/how-to-analyze-articles-about-data-science-using-data-science/\n",
    "<br><br>\n",
    "**Regular Expressions**\n",
    "- https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
    "- https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial\n",
    "- https://www.oreilly.com/ideas/an-introduction-to-regular-expressions\n",
    "\n",
    "\n",
    "**NLP Tutorials**\n",
    "\n",
    "- https://github.com/bonzanini/nlp-tutorial\n",
    "- https://github.com/totalgood/pycon-2016-nlp-tutorial\n",
    "\n",
    "**Text similarity:**\n",
    "- https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "- http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/\n",
    "- http://billchambers.me/tutorials/2014/12/22/cosine-similarity-explained-in-python.html\n",
    "- Explains why text similarity uses cosine similarity -> https://www.quora.com/What-are-the-mechanics-of-cosine-similarity-in-natural-language-processing\n",
    "\n",
    "**Text classification:**\n",
    "- Another fake news tutorial - > https://www.datacamp.com/community/tutorials/scikit-learn-fake-news\n",
    "- http://nlpforhackers.io/text-classification/\n",
    "- http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "- https://github.com/javedsha/text-classification\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    "\n",
    "\n",
    "**Text clustering:**\n",
    "\n",
    "- Great tutorial -> http://brandonrose.org/clustering\n",
    "- http://nlpforhackers.io/recipe-text-clustering/\n",
    "- https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "- http://mccormickml.com/2015/08/05/document-clustering-example-in-scikit-learn/\n",
    "\n",
    "\n",
    "**Word Embeddings/Word2Vec**\n",
    "\n",
    "- https://chatbotsmagazine.com/introduction-to-word-embeddings-55734fd7068a\n",
    "- https://www.springboard.com/blog/introduction-word-embeddings/\n",
    "- http://ruder.io/word-embeddings-1/\n",
    "- https://www.slideshare.net/BhaskarMitra3/a-simple-introduction-to-word-embeddings\n",
    "- https://github.com/fastai/word-embeddings-workshop\n",
    "\n",
    "\n",
    "**Topic Modeling**\n",
    "\n",
    "- http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "- https://blog.bigml.com/2016/11/16/introduction-to-topic-models/\n",
    "- http://nbviewer.jupyter.org/github/ogrisel/notebooks/blob/master/nmf_topics.ipynb?create=1\n",
    "- https://www.youtube.com/watch?v=ZgyA1Q2ywbM\n",
    "- https://www.youtube.com/watch?v=SjRss8Uk6mQ\n",
    "- https://github.com/derekgreene/topic-model-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab time\n",
    "\n",
    "Pick a text dataset to spend the rest of class working. There are three other datasets in the NLP_data that you can work with: pitchfork album reviews, fake/real news, deadspin, and political lean. Make sure to unzip political lean or fake news. You can also continue to work with the datasets we've already used (data science, yelp, spam.)\n",
    "\n",
    "<br>\n",
    "\n",
    "For the rest of class apply supervised or unsupervised learning techniques to the dataset of your choice. \n",
    "\n",
    "- Build a model that can differentiate between good/bad review, real/fake news, or liberal/conservative leaning or a model that \n",
    "\n",
    "- Predict how many page views a deadspin can get based on its headlines and tags.\n",
    "\n",
    "- Ignore the labels and attempt cluster the articles.\n",
    "\n",
    "- Have fun with the summarizer!!\n",
    "\n",
    "<br>\n",
    "\n",
    "Be prepared to share your results at the end of class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
